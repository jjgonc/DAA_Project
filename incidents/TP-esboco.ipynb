{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático\n",
    "\n",
    "## Linear Regression Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "**Import pandas, numpy, matplotlib,and seaborn. Then set %matplotlib inline \n",
    "(You'll import sklearn as you need it.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('training_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove City Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citiesValues = train.groupby(['city_name'])['city_name'].count()\n",
    "print(citiesValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['city_name'],axis=1)\n",
    "test = test.drop(['city_name'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude of delay treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude = train.groupby(['magnitude_of_delay'])['magnitude_of_delay'].count()\n",
    "test.groupby(['magnitude_of_delay'])['magnitude_of_delay'].count()\n",
    "print(magnitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['magnitude_of_delay'],axis=1)\n",
    "test = test.drop(['magnitude_of_delay'],axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay in Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def absolute_maximum_scale(series):\n",
    "#     return series / series.abs().max()\n",
    "\n",
    "\n",
    "# train['delay_in_seconds'] = absolute_maximum_scale(train['delay_in_seconds'])\n",
    "# test['delay_in_seconds'] = absolute_maximum_scale(test['delay_in_seconds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avg_Precipitaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation = train.groupby(['avg_precipitation'])['avg_precipitation'].count()\n",
    "print(precipitation)\n",
    "\n",
    "train = train.drop(['avg_precipitation'],axis=1)\n",
    "test = test.drop(['avg_precipitation'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Enconding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luminosity_values = train.groupby(['luminosity'])['luminosity'].count()\n",
    "print(luminosity_values)\n",
    "\n",
    "rain_values = train.groupby(['avg_rain'])['avg_rain'].count()\n",
    "print(rain_values)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "train['luminosity'] = label_encoder.fit_transform(train['luminosity'])\n",
    "train['avg_rain'] = label_encoder.fit_transform(train['avg_rain'])\n",
    "# train['delay_in_seconds'] = label_encoder.fit_transform(train['delay_in_seconds'])\n",
    "\n",
    "\n",
    "\n",
    "test['avg_rain'] = label_encoder.fit_transform(test['avg_rain'])\n",
    "test['luminosity'] = label_encoder.fit_transform(test['luminosity'])\n",
    "# test['delay_in_seconds'] = label_encoder.fit_transform(test['delay_in_seconds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affected Roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads = train.groupby(['affected_roads'])['affected_roads'].count()\n",
    "\n",
    "def hotLabelEncoding(dataset):\n",
    "    dict = {}\n",
    "    dict['total_estradas'] = []\n",
    "        \n",
    "    columns = dataset['affected_roads']\n",
    "\n",
    "    for column in columns:\n",
    "        if (column != '' and isinstance(column, str) ):\n",
    "            roadList = column.split(\",\")\n",
    "            for road in roadList:\n",
    "                if (road != '' and road != 'nan' and road != 'IC5 - N206' and road != 'N101 - N310' and road != 'N207-4'):\n",
    "                    dict[road] = []\n",
    "                    \n",
    "    \n",
    "    for line in dataset['affected_roads']:\n",
    "        if (line != '' and isinstance(line, str) ):\n",
    "            count = 0\n",
    "            total_count = 0\n",
    "            for road in dict.keys() :\n",
    "                if(road != 'total_estradas'):\n",
    "                    count = line.count(road)\n",
    "                    total_count = total_count + count\n",
    "                    dict[road].append(count)\n",
    "            dict['total_estradas'].append(total_count)\n",
    "        else:\n",
    "            for road in dict.keys():\n",
    "                dict[road].append(0)\n",
    "\n",
    "\n",
    "    for road in sorted(dict.keys(), key=lambda x:x.lower()):\n",
    "        if road == 'total_estradas':\n",
    "             dataset[road] = dict[road]\n",
    "    \n",
    "\n",
    "hotLabelEncoding(dataset=train)\n",
    "hotLabelEncoding(dataset=test)\n",
    "\n",
    "\n",
    "train = train.drop(['affected_roads'],axis=1)\n",
    "test = test.drop(['affected_roads'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and hour treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate collumn Date \n",
    "def separate_year(time):\n",
    "    res = re.search(r'(\\d+)-(\\d+)-(\\d+) (\\d+):(\\d+)', time)\n",
    "    if res:\n",
    "        return int(res.group(1))\n",
    "\n",
    "def separate_month(time):\n",
    "    res = re.search(r'(\\d+)-(\\d+)-(\\d+) (\\d+):(\\d+)', time)\n",
    "    if res:\n",
    "        return int(res.group(2))\n",
    "    \n",
    "def separate_day(time):\n",
    "    res = re.search(r'(\\d+)-(\\d+)-(\\d+) (\\d+):(\\d+)', time)\n",
    "    if res:\n",
    "        return int(res.group(3))\n",
    "    \n",
    "def separate_hour(time):\n",
    "    res = re.search(r'(\\d+)-(\\d+)-(\\d+) (\\d+):(\\d+)', time)\n",
    "    if res:\n",
    "        return int(res.group(4))\n",
    "        \n",
    "def separate_minutes(time):\n",
    "    res = re.search(r'(\\d+)-(\\d+)-(\\d+) (\\d+):(\\d+)', time)\n",
    "    if res:\n",
    "        return int(res.group(5))\n",
    "    \n",
    "train['Year'] = train['record_date'].apply(separate_year)\n",
    "train['Month'] = train['record_date'].apply(separate_month)\n",
    "train['Day'] = train['record_date'].apply(separate_day)\n",
    "train['Hour'] = train['record_date'].apply(separate_hour)\n",
    "train['Minutes'] = train['record_date'].apply(separate_minutes)\n",
    "minutes_values = train.groupby(['Minutes'])['Minutes'].count()\n",
    "print(minutes_values)\n",
    "\n",
    "train = train.drop(['record_date'],axis=1)\n",
    "\n",
    "\n",
    "test['Year'] = test['record_date'].apply(separate_year)\n",
    "test['Month'] = test['record_date'].apply(separate_month)\n",
    "test['Day'] = test['record_date'].apply(separate_day)\n",
    "test['Hour'] = test['record_date'].apply(separate_hour)\n",
    "test['Minutes'] = test['record_date'].apply(separate_minutes)\n",
    "test = test.drop(['record_date'],axis=1)\n",
    "\n",
    "test = test.drop(['Minutes'],axis=1)\n",
    "train = train.drop(['Minutes'],axis=1)\n",
    "\n",
    "test = test.drop(['Year'],axis=1)\n",
    "train = train.drop(['Year'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Média estradas afetadas por gravidade de incidente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.barplot(data=train, x=\"incidents\", y=\"total_estradas\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hora com mais incidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train, x=\"Hour\", hue=\"incidents\", multiple=\"dodge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mês com mais incidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train, x=\"Month\", hue=\"incidents\", multiple=\"dodge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delay pelo número de estradas afetadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=train, x=\"total_estradas\", y=\"delay_in_seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train.corr()\n",
    "f , ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, vmin=-1, vmax=1, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = train.drop(['incidents'], axis=1)\n",
    "y = train['incidents']\n",
    "\n",
    "X.to_csv('X.csv')\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = skl.model_selection.train_test_split(X,y, test_size=0.3, random_state=2022)   # using 30% for testing and 70% for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos entao dar fit nos dados de treino e prever com os de teste\n",
    "lg = LogisticRegression(max_iter=100000,random_state=2022)\n",
    "lg.fit(x_train, y_train)    #eventualmente podemos mudar o solver!\n",
    "predictions = lg.predict(x_test)\n",
    "score = lg.score(x_test, y_test)\n",
    "print('\\n\\n\\nScore with train data: ' + str(score) + '\\n\\n')\n",
    "plot_confusion_matrix(lg, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=1000, random_state=2022)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(lg, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "model = CatBoostClassifier(iterations=700,\n",
    "                          learning_rate=1,\n",
    "                          depth=4,\n",
    "                          random_state=2022)\n",
    "# Fit model\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_pred=model.predict(x_test)\n",
    "# Get predictions\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(lg, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cls = DecisionTreeClassifier()\n",
    "\n",
    "model = BaggingClassifier(base_estimator = base_cls,\n",
    "                          n_estimators = 1000,\n",
    "                          random_state = 2022,\n",
    "                          max_samples = 0.7)\n",
    "\n",
    "\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Get predictions\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "plot_confusion_matrix(lg, x_test, y_test)\n",
    "\n",
    "predictions = model.predict(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "res = pd.DataFrame(data={'RowId': range(1,1207), 'Incidents': predictions})\n",
    "res.to_csv('res.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('daa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "26f37f753ca6524bd3015df771a4bdb5cf1f756fdaa64b50eb2729c22f292f2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
